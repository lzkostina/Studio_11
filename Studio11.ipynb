{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "91125c74-712e-452b-ba94-40e294b31002",
   "metadata": {},
   "source": [
    "**Name:**\n",
    "\n",
    "**Collaborator(s):**\n",
    "\n",
    "\n",
    "# Studio 11 - The Folk Theorem\n",
    "\n",
    "Gelman's [Folk Theorem of Statistical Computing](https://statmodeling.stat.columbia.edu/2008/05/13/the_folk_theore/) states\n",
    "\n",
    "> _\"When you have computational problems, often thereâ€™s a problem with your model.\"_\n",
    "\n",
    "This principle means that computational difficulties (slow convergence, sensitivity to initialization, getting stuck in local optima) can reflect genuine statistical issues (weak identifiability, overparameterization, insufficient data). Today we will see an example of this; along the way, we'll practice making an algorithm more efficient and numerically stable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2624c90e-ed6f-4e88-9c67-93f5db39f3d8",
   "metadata": {},
   "source": [
    "### Defining a Gaussian Mixture Model\n",
    "\n",
    "A Gaussian Mixture Model (GMM) represents data as coming from a mixture of $K$ Gaussian distributions. The density of a GMM has the form:\n",
    "$$p(\\mathbf{x}) = \\sum_{k=1}^K\\pi_k \\mathcal{N}(\\mathbf{x} | \\boldsymbol{\\mu}_k, \\boldsymbol{\\Sigma}_k),$$\n",
    "where\n",
    "- $\\pi_k$ are mixing proportions (with $\\pi_k\\ge 0$ and $\\sum_{k=1}^K\\pi_k = 1$)\n",
    "- $\\boldsymbol{\\mu}_k \\in \\mathbb{R}^d$ are component means\n",
    "- $\\boldsymbol{\\Sigma}_k \\in \\mathbb{R}^{d\\times d}$ are component covariance matrices (must be positive definite)\n",
    "\n",
    "We have a sample of observations $(x_i)_{i=1}^n$ and wish to estimate the parameters of the GMM.\n",
    "\n",
    "### Latent Variable Interpretation\n",
    "\n",
    "If $x_i$ is drawn from the GMM $p$, we can introduce a latent variable $z_i \\in {1, 2, ..., K}$ that indicates which Gaussian distribution $\\mathbf{x}_i$ came from, i.e.\n",
    "- $p(z_i = k) = \\pi_k$ (prior probability of component k)\n",
    "- $p(\\mathbf{x}_i | z_i = k) = \\mathcal{N}(\\mathbf{x}_i | \\boldsymbol{\\mu}_k, \\boldsymbol{\\Sigma}_k)$ (likelihood given component)\n",
    "\n",
    "If we knew the $z_i$ values, parameter estimation would be easy: just compute sample means and covariances for each group! Since we don't know them, the _EM algorithm_ alternates between inferring the latent assignments and updating the parameters as if those inferred assignments were correct."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "785b2b55-2a76-49c3-b275-6f34b4ebc992",
   "metadata": {},
   "source": [
    "## The EM Algorithm\n",
    "\n",
    "EM alternates between two steps:\n",
    "### E-Step (Expectation)\n",
    "\n",
    "Compute the posterior \"responsibility\" that component $k$ takes for observation $i$:\n",
    "$$\\gamma_{ik} = p(z_i = k | \\mathbf{x}_i) = \\frac{\\pi_k \\mathcal{N}(\\mathbf{x}_i | \\boldsymbol{\\mu}_k, \\boldsymbol{\\Sigma}_k)}{\\sum_{j=1}^K \\pi_j \\mathcal{N}(\\mathbf{x}_i | \\boldsymbol{\\mu}_j, \\boldsymbol{\\Sigma}_j)}$$\n",
    "Responsibilities are soft cluster assignments: each point belongs partially to each component.\n",
    "\n",
    "### M-Step (Maximization)\n",
    "\n",
    "Update parameters using the weighted data:\n",
    "- **Mixing proportions:** $\\pi_k = \\frac{1}{n} \\sum_{i=1}^n \\gamma_{ik}$\n",
    "- **Means:** $\\boldsymbol{\\mu}_k = \\frac{\\sum_{i=1}^n \\gamma_{ik} \\mathbf{x}_i}{\\sum_{i=1}^n \\gamma_{ik}}$\n",
    "- **Covariances:** $\\boldsymbol{\\Sigma}_k = \\frac{\\sum_{i=1}^n \\gamma_{ik} (\\mathbf{x}_i - \\boldsymbol{\\mu}_k)(\\mathbf{x}_i - \\boldsymbol{\\mu}_k)^T}{\\sum_{i=1}^n \\gamma_{ik}}$\n",
    "### Algorithm Summary\n",
    "\n",
    "Initialize parameters $\\theta^{(0)} = (\\pi^{(0)}, \\mu^{(0)}, \\Sigma^{(0)})$ randomly\n",
    "\n",
    "Repeat:\n",
    "-  E-step: compute responsibilities $\\gamma^{(t+1)}$ given $\\theta^{(t)}$\n",
    "-  M-step: update $\\theta^{(t+1)}$ using responsibilities $\\gamma^{(t+1)}$\n",
    "\n",
    "**Convergence criterion**: Stop when the log-likelihood change is smaller than some threshold $\\varepsilon$, or when some maximum number of iterations has been reached."
   ]
  },
  {
   "cell_type": "code",
   "id": "a9b1e6e3-18aa-4fa1-99b2-d79d09d1aa91",
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2025-11-06T16:37:02.503981Z",
     "start_time": "2025-11-06T16:37:02.373422Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "from naive_em_algorithm import GaussianMixtureModel\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Ellipse\n",
    "plt.rcParams.update({\n",
    "    'lines.linewidth' : 1.,\n",
    "    'lines.markersize' : 5,\n",
    "    'font.size': 9,\n",
    "    \"text.usetex\": True,\n",
    "    'font.family': 'serif', \n",
    "    'font.serif': ['Computer Modern'],\n",
    "    'text.latex.preamble' : r'\\usepackage{amsmath,amsfonts}',\n",
    "    'axes.linewidth' : .75})\n",
    "\n",
    "from time import time"
   ],
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'numpy'",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mModuleNotFoundError\u001B[39m                       Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[1]\u001B[39m\u001B[32m, line 1\u001B[39m\n\u001B[32m----> \u001B[39m\u001B[32m1\u001B[39m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mnumpy\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mas\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mnp\u001B[39;00m\n\u001B[32m      2\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mnaive_em_algorithm\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m GaussianMixtureModel\n\u001B[32m      4\u001B[39m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mmatplotlib\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mpyplot\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mas\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mplt\u001B[39;00m\n",
      "\u001B[31mModuleNotFoundError\u001B[39m: No module named 'numpy'"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "id": "744f4d6e-f831-4a99-9bb3-aa75b43a2fc5",
   "metadata": {},
   "source": [
    "## Exercise 1: Vectorize the EM Algorithm\n",
    "\n",
    "Explore the implementation of the EM Algorithm in `naive_em_algorithm.py`. Then, implement your own class, called `GaussianMixtureModelVectorized`, in the file `vectorized_em_algorithm.py`. Your code should be at least 100 times faster than the naive implementation on the example below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea8234d4-b31a-4572-8ae6-5b1ec88c3bd6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "n_components = 2\n",
    "pi = np.ones(n_components) / n_components\n",
    "mus = [np.array([0., 0.]), np.array([2., 2.])]\n",
    "covs = [np.eye(2)]*2\n",
    "\n",
    "DGP = GaussianMixtureModel(random_state=1234567891)\n",
    "DGP._set_parameters(mus, covs, pi)\n",
    "\n",
    "n_samples = 200\n",
    "X, Z = DGP.sample(n_samples)\n",
    "\n",
    "plt.figure(figsize=(5,5))\n",
    "plt.plot(X[:,0], X[:, 1], '.', color='gray', alpha=.3)\n",
    "DGP.plot_2D_model(plt.gca(), 'br')\n",
    "plt.title('Data and True GMM Parameters')\n",
    "plt.xlim([-3, 5])\n",
    "plt.ylim([-3, 5])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee235ce9-65d9-4802-ba88-187dda2ff342",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "t0 = time()\n",
    "indices = np.random.choice(n_samples, n_components, replace=False)\n",
    "initial_theta = [[X[i].copy() for i in indices],\n",
    "    [np.eye(2) for _ in range(n_components)],\n",
    "    np.ones(n_components) / n_components]\n",
    "GMM = GaussianMixtureModel()\n",
    "GMM.fit(X, initial_theta=initial_theta)\n",
    "t1 = time()\n",
    "naive_time = t1-t0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95808935-47f7-43bb-8f90-257ba6744ced",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 2, figsize=(12,6))\n",
    "axs[0].plot(X[:,0], X[:, 1], '.', color='gray', alpha=.3)\n",
    "GMM.plot_2D_model(axs[0], 'br')\n",
    "axs[0].set_title('Data and Estimated Parameters')\n",
    "axs[0].set_xlim([-3, 5])\n",
    "axs[0].set_ylim([-3, 5])\n",
    "\n",
    "axs[1].plot(GMM.log_likelihoods_, c='k')\n",
    "axs[1].set_title('Log-likelihood vs. Iteration')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d3fee7f-77a7-4c7d-a613-d49e31bbb852",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# TO DO: implement this before running!\n",
    "from vectorized_em_algorithm import GaussianMixtureModelVectorized\n",
    "\n",
    "t0 = time()\n",
    "GMM2 = GaussianMixtureModelVectorized()\n",
    "GMM2.fit(X, initial_theta=initial_theta)\n",
    "t1 = time()\n",
    "vectorized_time = t1-t0\n",
    "\n",
    "print(f\"Vectorized code was {naive_time / vectorized_time:.2f} times faster!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4159de4-4098-4a40-934d-785874eb9722",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "assert np.allclose(GMM.mu_, GMM2.mu_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "940fe265-037a-4fb1-bcdf-157a46088e0e",
   "metadata": {},
   "source": [
    "## Exercise 2: Make the Method Numerically Stable\n",
    "\n",
    "Recall from lecture that the normalizing constant $\\sum_{j=1}^K \\pi_j \\mathcal{N}(\\mathbf{x}_i | \\boldsymbol{\\mu}_j, \\boldsymbol{\\Sigma}_j)$ in the responsibilities can be very small if $\\mathbf{x}_i$ is far from every cluster. This leads to divide by zero errors in the E-step.\n",
    "\n",
    "Implement the log-sum-exp trick in `GaussianMixtureModelVectorized` to compute both the responsibilities in `_e_step` and the log-likelihood in `_compute_log_likelihood`. These will both be necessary for Exercise 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d81dc4dc-4e1a-4b88-8ca8-3e5f1c49fd0a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "n_features = 50\n",
    "n_components = 2\n",
    "pi = np.ones(n_components) / n_components\n",
    "covs = [np.eye(n_features)]*2\n",
    "\n",
    "for scale in [.5, 1, 5, 50]:\n",
    "    mus = [-scale*np.random.rand(n_features), scale*np.random.rand(n_features)]\n",
    "\n",
    "    DGP = GaussianMixtureModel(random_state=1234567891)\n",
    "    DGP._set_parameters(mus, covs, pi)\n",
    "\n",
    "    n_samples = 200\n",
    "    X, Z = DGP.sample(n_samples)\n",
    "\n",
    "    GMM = GaussianMixtureModel()\n",
    "    GMM2 = GaussianMixtureModelVectorized()\n",
    "\n",
    "    GMM._set_parameters([np.zeros(n_features)]*2, covs, pi)\n",
    "    GMM2._set_parameters([np.zeros(n_features)]*2, covs, pi)\n",
    "    print(GMM.score(X), GMM2.score(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bb8e243-5a82-48fb-8411-491574ce721d",
   "metadata": {},
   "source": [
    "## Exercise 3: Document the Folk Theorem\n",
    "\n",
    "The model identifiability of a GMM depends on the cluster separation, e.g. $\\|\\mu_1-\\mu_2\\|$. When mixture components are harder to distinguish statistically (small separation), the Folk Theorem suggests EM will run into a range of problems:\n",
    "- More iterations/more time to converge (slow convergence)\n",
    "- Higher variance in convergence time across initializations (sensitivity to initialization)\n",
    "- Greater sensitivity e.g. in final log-likelihood to random initialization (getting stuck in local optima)\n",
    "\n",
    "Generate datasets with varying separations, run EM multiple times (e.g. 20-30 runs) at each separation with different random initializations, and plot each of these metrics as a function of separation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a19a1197-8660-4d31-ab10-5260ac14488f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data_with_separation(separation, n_samples=200, n_features=2, random_state=None):\n",
    "    \"\"\"\n",
    "    Generate 2D data from two Gaussian components with specified separation.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    separation : float\n",
    "        Distance between component means (Euclidean distance)\n",
    "    n_samples : int, default=200\n",
    "        Total number of samples to generate\n",
    "    n_features : int, default=2\n",
    "        Dimensionality\n",
    "    random_state : int or None, default=None\n",
    "        Random seed for reproducibility\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    X : array, shape (n_samples, 2)\n",
    "        Generated data points\n",
    "    true_labels : array, shape (n_samples,)\n",
    "        True component assignments (0 or 1)\n",
    "    \"\"\"\n",
    "    # Create a GMM with known parameters\n",
    "    DGP = GaussianMixtureModelVectorized(n_components)\n",
    "    \n",
    "    # Set true parameters manually\n",
    "    mu1 = np.zeros(n_features)\n",
    "    mu2 = separation * np.ones(n_features) / np.sqrt(n_features)\n",
    "    \n",
    "    cov = np.eye(n_features)\n",
    "    \n",
    "    DGP.mu_ = np.array([mu1, mu2])\n",
    "    DGP.cov_ = np.array([cov]*2)\n",
    "    DGP.pi_ = np.array([0.5, 0.5])\n",
    "    \n",
    "    # Sample from the GMM\n",
    "    X, Z = DGP.sample(n_samples=n_samples, random_state=random_state)\n",
    "    \n",
    "    return X, Z"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
